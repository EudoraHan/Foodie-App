<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Yun Han, Juehao Hu, Diyang Wu, Edward Yu, Xinrui Zhang" />


<title>Biostat M280 Homework 4</title>

<script src="hw4sol_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="hw4sol_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="hw4sol_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="hw4sol_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="hw4sol_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="hw4sol_files/navigation-1.1/tabsets.js"></script>
<link href="hw4sol_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="hw4sol_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->





<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Biostat M280 Homework 4</h1>
<h3 class="subtitle"><em>Due Mar 22 @ 11:59PM</em></h3>
<h4 class="author"><em>Yun Han, Juehao Hu, Diyang Wu, Edward Yu, Xinrui Zhang</em></h4>

</div>


<div id="q1-learn-by-doing" class="section level2">
<h2>Q1 Learn by doing</h2>
<div id="q1.1-reproduce-the-results-in-the-blog" class="section level3">
<h3>Q1.1 Reproduce the results in the blog</h3>
<p>Our group searched through the R blog’s website for an interesting project to undertake. After combing through several blogs, our group settled on the Neural style transfer with eager execution and Keras’(<a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer/" class="uri">https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer/</a>). The code uses neural style transfer techniques to transmute an original picture into the ‘style’ of a second picture. The model that the blog utilizes is a simplied version of model <code>VGG19</code>, which is a trained model on <code>ImageNet</code>. The blog modifies the <code>VGG19</code> model by computing <code>loss function</code> and back propagating it to get the direction of changes that we want. Specific explanations of the model functionality will be provided later in each step. In order to run the the code properly, we modified the original code on the blog since the original code contained errors. We added comments into the code section to explain each section of the code:</p>
<pre class="r"><code>#installing necessary packages
install.packages(c(&quot;tensorflow&quot;, &quot;keras&quot;, &quot;tfdatasets&quot;))

library(tensorflow)
library(tfdatasets)
library(keras)
library(purrr)
library(glue)
library(reticulate)
use_implementation(&quot;tensorflow&quot;)
tfe_enable_eager_execution(device_policy = &quot;silent&quot;)</code></pre>
<p>The <code>img_shape</code> determines the resolution dimensions of <code>content_image</code> and <code>style_image</code>. The original blog post uses resolution 128x128. Here we consider resolution as one of our tuning parameters. We will compare and contrast different resolution qualities and its effect on the final trasmuted image.</p>
<pre class="r"><code>img_shape &lt;- c(256, 256, 3)</code></pre>
<p>We load the original content image:</p>
<pre class="r"><code>library(tensorflow)
library(tfdatasets)
library(keras)
library(purrr)
library(glue)
library(reticulate)
content_path &lt;-&quot;~/biostatm280-winter2019-hw4/Q1/originalRendition/GreatWave/isar.jpg&quot;

content_image &lt;-  image_load(content_path, target_size = img_shape[1:2])
content_image %&gt;% 
  image_to_array() %&gt;%
  `/`(., 255) %&gt;%
  as.raster() %&gt;%
  plot()</code></pre>
<p>Now we load the style image:</p>
<pre class="r"><code>style_path &lt;- &quot;~/biostatm280-winter2019-hw4/Q1/originalRendition/GreatWave/The_Great_Wave_off_Kanagawa.jpg&quot;

style_image &lt;-  image_load(style_path, target_size = img_shape[1:2])
style_image %&gt;% 
  image_to_array() %&gt;%
  `/`(., 255) %&gt;%
  as.raster() %&gt;%
  plot()</code></pre>
<p>Create the wrapper that loads and preprocesses the input images. The model that we employ here is <code>VGG19</code>, a network that has been trained on <code>ImageNet</code>.</p>
<pre class="r"><code>#create a wrapper
load_and_process_image &lt;- function(path) {
  img &lt;- image_load(path, target_size = img_shape[1:2]) %&gt;%
    image_to_array() %&gt;%
    k_expand_dims(axis = 1) %&gt;%
    imagenet_preprocess_input()
}

deprocess_image &lt;- function(x) {
  x &lt;- x[1, , ,]
  # Remove zero-center by mean pixel
  x[, , 1] &lt;- x[, , 1] + 103.939
  x[, , 2] &lt;- x[, , 2] + 116.779
  x[, , 3] &lt;- x[, , 3] + 123.68
  # &#39;BGR&#39;-&gt;&#39;RGB&#39;
  x &lt;- x[, , c(3, 2, 1)]
  x[x &gt; 255] &lt;- 255
  x[x &lt; 0] &lt;- 0
  x[] &lt;- as.integer(x) / 255
  x
}</code></pre>
<p>Now we explore the layers of the neural network. Within this model, there is no training, but a neural style transfer back propagates the loss to the input layer to get closer to our desired style image. There are two primary layers content and style. The content layer is compared to the style layer via loss function. There are 5 style layers are analagous to level features such as texture, shapes, strokes, etc:</p>
<pre class="r"><code>#setting the scene
content_layers &lt;- c(&quot;block5_conv2&quot;)
style_layers &lt;- c(&quot;block1_conv1&quot;,
                  &quot;block2_conv1&quot;,
                  &quot;block3_conv1&quot;,
                  &quot;block4_conv1&quot;,
                  &quot;block5_conv1&quot;)

num_content_layers &lt;- length(content_layers)
num_style_layers &lt;- length(style_layers)

get_model &lt;- function() {
  vgg &lt;- application_vgg19(include_top = FALSE, weights = &quot;imagenet&quot;)
  vgg$trainable &lt;- FALSE
  style_outputs &lt;- map(style_layers, function(layer) vgg$get_layer(layer)$output)
  content_outputs &lt;- map(content_layers, function(layer) vgg$get_layer(layer)$output)
  model_outputs &lt;- c(style_outputs, content_outputs)
  keras_model(vgg$input, model_outputs)
}</code></pre>
<p>This model controls for three types of losses: content, style, and regularization loss.</p>
<pre class="r"><code>#define losses
content_loss &lt;- function(content_image, target) {
  k_sum(k_square(target - content_image))
}

gram_matrix &lt;- function(x) {
  features &lt;- k_batch_flatten(k_permute_dimensions(x, c(3, 1, 2)))
  gram &lt;- k_dot(features, k_transpose(features))
  gram
}

style_loss &lt;- function(gram_target, combination) {
  gram_comb &lt;- gram_matrix(combination)
  k_sum(k_square(gram_target - gram_comb)) /
    (4 * (img_shape[3] ^ 2) * (img_shape[1] * img_shape[2]) ^ 2)
}

total_variation_loss &lt;- function(image) {
  y_ij  &lt;- image[1:(img_shape[1] - 1L), 1:(img_shape[2] - 1L),]
  y_i1j &lt;- image[2:(img_shape[1]), 1:(img_shape[2] - 1L),]
  y_ij1 &lt;- image[1:(img_shape[1] - 1L), 2:(img_shape[2]),]
  a &lt;- k_square(y_ij - y_i1j)
  b &lt;- k_square(y_ij - y_ij1)
  k_sum(k_pow(a + b, 1.25))
}

content_weight &lt;- 100
style_weight &lt;- 0.8
total_variation_weight &lt;- 0.01</code></pre>
<p>Here we get the model outputs for content and style images:</p>
<pre class="r"><code>#get model output for content and style 
get_feature_representations &lt;-
  function(model, content_path, style_path) {
    
    # dim == (1, 128, 128, 3)
    style_image &lt;-
      load_and_process_image(style_path) %&gt;% k_cast(&quot;float32&quot;)
    # dim == (1, 128, 128, 3)
    content_image &lt;-
      load_and_process_image(content_path) %&gt;% k_cast(&quot;float32&quot;)
    # dim == (2, 128, 128, 3)
    stack_images &lt;- k_concatenate(list(style_image, content_image), axis = 1)
    
    # length(model_outputs) == 6
    # dim(model_outputs[[1]]) = (2, 128, 128, 64)
    # dim(model_outputs[[6]]) = (2, 8, 8, 512)
    model_outputs &lt;- model(stack_images)
    
    style_features &lt;- 
      model_outputs[1:num_style_layers] %&gt;%
      map(function(batch) batch[1, , , ])
    content_features &lt;- 
      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)] %&gt;%
      map(function(batch) batch[2, , , ])
    
    list(style_features, content_features)
  }</code></pre>
<p>Here we compute the losses for every loss type and gradient for the overall loss with respect to the initial image input:</p>
<pre class="r"><code>#compute loss
compute_loss &lt;-
  function(model, loss_weights, init_image, gram_style_features, content_features) {
    
    c(style_weight, content_weight) %&lt;-% loss_weights
    model_outputs &lt;- model(init_image)
    style_output_features &lt;- model_outputs[1:num_style_layers]
    content_output_features &lt;-
      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)]
    
    # style loss
    weight_per_style_layer &lt;- 1 / num_style_layers
    style_score &lt;- 0
    # dim(style_zip[[5]][[1]]) == (512, 512)
    style_zip &lt;- transpose(list(gram_style_features, style_output_features))
    for (l in 1:length(style_zip)) {
      # for l == 1:
      # dim(target_style) == (64, 64)
      # dim(comb_style) == (1, 128, 128, 64)
      c(target_style, comb_style) %&lt;-% style_zip[[l]]
      style_score &lt;- style_score + weight_per_style_layer * 
        style_loss(target_style, comb_style[1, , , ])
    }
    
    # content loss
    weight_per_content_layer &lt;- 1 / num_content_layers
    content_score &lt;- 0
    content_zip &lt;- transpose(list(content_features, content_output_features))
    for (l in 1:length(content_zip)) {
      # dim(comb_content) ==  (1, 8, 8, 512)
      # dim(target_content) == (8, 8, 512)
      c(target_content, comb_content) %&lt;-% content_zip[[l]]
      content_score &lt;- content_score + weight_per_content_layer *
        content_loss(comb_content[1, , , ], target_content)
    }
    
    # total variation loss
    variation_loss &lt;- total_variation_loss(init_image[1, , ,])
    
    style_score &lt;- style_score * style_weight
    content_score &lt;- content_score * content_weight
    variation_score &lt;- variation_loss * total_variation_weight
    
    loss &lt;- style_score + content_score + variation_score
    list(loss, style_score, content_score, variation_score)
  }

#compute gradients
compute_grads &lt;- 
  function(model, loss_weights, init_image, gram_style_features, content_features) {
    with(tf$GradientTape() %as% tape, {
      scores &lt;-
        compute_loss(model,
                     loss_weights,
                     init_image,
                     gram_style_features,
                     content_features)
    })
    total_loss &lt;- scores[[1]]
    list(tape$gradient(total_loss, init_image), scores)
  }</code></pre>
<p>Below, the style and content features are calculated just once, but are then iterated over epochs. An output is created every 100 epochs until 500 epochs is reached.</p>
<pre class="r"><code>#run the model/training phase
run_style_transfer &lt;- function(content_path, style_path) {
  model &lt;- get_model()
  walk(model$layers, function(layer) layer$trainable = FALSE)
  
  c(style_features, content_features) %&lt;-% 
    get_feature_representations(model, content_path, style_path)
  # dim(gram_style_features[[1]]) == (64, 64)
  gram_style_features &lt;- map(style_features, function(feature) gram_matrix(feature))
  
  init_image &lt;- load_and_process_image(content_path)
  init_image &lt;- tf$contrib$eager$Variable(init_image, dtype = &quot;float32&quot;)
  
  optimizer &lt;- tf$train$AdamOptimizer(learning_rate = 1,
                                      beta1 = 0.99,
                                      epsilon = 1e-1)
  
  c(best_loss, best_image) %&lt;-% list(Inf, NULL)
  loss_weights &lt;- list(style_weight, content_weight)
  
  start_time &lt;- Sys.time()
  global_start &lt;- Sys.time()
  
  norm_means &lt;- c(103.939, 116.779, 123.68)
  min_vals &lt;- -norm_means
  max_vals &lt;- 255 - norm_means
  num_iterations &lt;- 500
  
  for (i in seq_len(num_iterations)) {
    # dim(grads) == (1, 128, 128, 3)
    c(grads, all_losses) %&lt;-% compute_grads(model,
                                            loss_weights,
                                            init_image,
                                            gram_style_features,
                                            content_features)
    c(loss, style_score, content_score, variation_score) %&lt;-% all_losses
    optimizer$apply_gradients(list(tuple(grads, init_image)))
    clipped &lt;- tf$clip_by_value(init_image, min_vals, max_vals)
    init_image$assign(clipped)
    
    end_time &lt;- Sys.time()
    
    if (k_cast_to_floatx(loss) &lt; best_loss) {
      best_loss &lt;- k_cast_to_floatx(loss)
      best_image &lt;- init_image
    }
    
    if (i %% 50 == 0) {
      glue(&quot;Iteration: {i}&quot;) %&gt;% print()
      glue(
        &quot;Total loss: {k_cast_to_floatx(loss)},
        style loss: {k_cast_to_floatx(style_score)},
        content loss: {k_cast_to_floatx(content_score)},
        total variation loss: {k_cast_to_floatx(variation_score)},
        time for 1 iteration: {(Sys.time() - start_time) %&gt;% round(2)}&quot;
      ) %&gt;% print()
      
      if (i %% 100 == 0) {
        png(paste0(&quot;style_epoch_&quot;, i, &quot;.png&quot;))
        plot_image &lt;- best_image$numpy()
        plot_image &lt;- deprocess_image(plot_image)
        plot(as.raster(plot_image), main = glue(&quot;Iteration {i}&quot;))
        dev.off()
      }
    }
  }
  
  glue(&quot;Total time: {Sys.time() - global_start} seconds&quot;) %&gt;% print()
  list(best_image, best_loss)
  }</code></pre>
<p>Now we execute the entire process!</p>
<pre class="r"><code>c(best_image, best_loss) %&lt;-% run_style_transfer(content_path, style_path)</code></pre>
</div>
<div id="q1.2-make-our-own-tweaks." class="section level3">
<h3>Q1.2 Make our own tweaks.</h3>
<p>We’ll now explore three types of analyses by resolution, epochs, and style changes.</p>
<p>We’ve already seen the original construction of the final transmuted image using a 128x128 resolution. The final image quality was quite poor, making important stylistic features unable to discern.</p>
<p>For the same 500 epoch generated image our 128x128 resolution is:</p>
<div class="figure">
<img src="~/biostatm280-winter2019-hw4/Q1/originalRendition/style_epoch_500.png" />

</div>
<p>For the same 500 epoch generated image our 512x512 resolution is:</p>
<div class="figure">
<img src="~/biostatm280-winter2019-hw4/Q1/originalRendition/style_epoch_500.png" />

</div>
<p>Although not a 1 to 1 comparison, for the sake of presentation we now examine the highest possible quality (using our limited GPU processing power) on a different image at 512x512 resolution. Our content image uses the famous painter Albert Biertstadt’s painting of the Rocky Mountains:</p>
<div class="figure">
<img src="~/biostatm280-winter2019-hw4/Q1/AlbertBierstadt.jpg" />

</div>
<p>We transform the original picture with Van Gogh’s Starry night as the style:</p>
<div class="figure">
<img src="~/biostatm280-winter2019-hw4/Q1/albert_epoch_500.png" />

</div>
</div>
<div id="epoch" class="section level3">
<h3>Epoch</h3>
<p>Comparisons can also be made on the output images based on the same base and style images, but with different epochs. Here we’ll show a <code>GIF</code> that puts together a set of output images from 100, 200, 300, 400, and 500 epochs respectively. The gif starts with images generated from 100 epochs, whose base hue is darker in general and contents not as clear as the original pictures. As the epoch increases, images tend to have a base hue closer to the style image and the contents more substantial.</p>
<div class="figure">
<img src="~/biostatm280-winter2019-hw4/Q1/output_FDmUpr.gif" />

</div>
</div>
<div id="style" class="section level3">
<h3>Style</h3>
<p>Five different styles were compiled at 256x256 resolution. We chose five distinctive styles so that the styles can readily be seen. We can see the texture differences particularly evident in the sketch style, where the black lines ripple across the Yosemite sky. The original content image of Chinese ink style is drawn in shades of grey, which results in the more muted color scales of the transformed picture. If we compare Monet’s and Van Gogh’s styles, we can even see the particular differences in brush stroke styles, color compositions, and textures. If Van Gogh and Monet were to draw Yosemite, I’m confident they’d both admire each other’s work.</p>
<div class="figure">
<img src="~/biostatm280-winter2019-hw4/Q1/transformed.png" />

</div>
</div>
<div id="work-distribution" class="section level3">
<h3>Work distribution</h3>
<ol style="list-style-type: decimal">
<li><p>Yun Han and Xinrui Zhang performed the majority of the blog reviews to select which blog would be interesting to reproduce. The rest of the group agreed on the blog choice.</p></li>
<li><p>Edward Yu, Diyang Wu, and Juehao Hu compiled the code from the blog. After recognizing that the code directly pulled form the blog did not run properly, we fixed bugs and added initial parameter values such as setting an <code>num_iterations</code> value.</p></li>
<li><p>Yun Han and Xinrui Zhang began the writing process for the reproduction section, helping to add comments to the code.</p></li>
<li><p>Juehao Hu performed the analysis of resolution section, showing the difference between 128x128 resolution and 512x512 resolution.</p></li>
<li><p>Diyang Wu performed the epoch section, incorporating a unique way to show the changes in epoch by compiling 5 images into a gif format.</p></li>
<li><p>Edward Yu performed the style section and created the transformed final image, laying out an easy way to compare how each style image creates its own version of the Yosemite picture. He also helped write the rmarkdown file. Other members contributed to selecting the style photos and uploading the photos into Github.</p></li>
<li><p>Yun Han summarized our final findings from Q1 and Q2 into the readme, highlighting the group’s hard work!</p></li>
</ol>
</div>
</div>
<div id="q2-deep-learning-on-smart-phone" class="section level2">
<h2>Q2 Deep learning on smart phone</h2>
<div id="app-description" class="section level3">
<h3>App Description</h3>
<p>This App allows users to detect 101 types of foods. Every result will be recorded so that the users can see what their children ate for the whole day. From the demo video, you can see a single result for food is shown at the top area, and all results will be recorded right below the picture.</p>
</div>
<div id="app-handbook" class="section level3">
<h3>App Handbook</h3>
<p><strong>Required Environment</strong></p>
<p>To smoothly run the App, <code>MacOS Mojave</code> (i.e <code>10.14</code> and above), and <code>Xcode 10</code> are required.</p>
<p><strong>Steps to Run</strong></p>
<p>Open the <code>Foodie.xcodeproj</code> click run to launch the simulator, after that, you can directly drag some pictures from the folder <code>testing picture</code> and have a try :).</p>
<p><strong>Reference</strong></p>
<p>We tried to find some reference from the Internet and found an excellent online course here(<a href="https://www.udemy.com/ios-12-app-development-bootcamp/learn/v4/overview" class="uri">https://www.udemy.com/ios-12-app-development-bootcamp/learn/v4/overview</a>) instructing how to develop a ios App by Xcode.</p>
</div>
<div id="work-distribution-1" class="section level3">
<h3>Work distribution</h3>
<ol style="list-style-type: decimal">
<li><p>Yun Han first construct a structure for the App.</p></li>
<li><p>Xinrui Zhang refined the structure, and then merge a trianed coreML model <code>InceptionV3</code> in to achieve the photo recognition.</p></li>
<li><p>Edward Yu optimized the user interface to make sure the selected images are not going to be twisted and looks good on screen of any size.</p></li>
<li><p>Diyang Wu then found a new coreML model <code>Food101</code> and merge that model in, now the model can detect 101 types of food.</p></li>
<li><p>The output for this new model is not good, for example, “French Fries” is presented as “French_Fries”, Juehao Hu fixed this problem.</p></li>
<li><p>Yun Han desing a new interface with a <code>record</code> part and Xinrui Zhang write code to achieve it.</p></li>
</ol>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
